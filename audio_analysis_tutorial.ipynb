{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio_analysis_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuomaseerola/audio/blob/master/audio_analysis_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTO_mMkGJCci"
      },
      "source": [
        "# Music and Science – Audio Analysis Tutorial\n",
        "\n",
        "[Tuomas Eerola](https://www.durham.ac.uk/staff/tuomas-eerola/), Durham University, Music Department, 2025.\n",
        "\n",
        "Audio examples adapted from the [FMP Notebooks](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C0/C0.html) by Meinhard Müller, which are part of [Fundamentals of Music Processing](https://www.audiolabs-erlangen.de/fau/professor/mueller/bookFMP).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3LSkxYNS5nY",
        "collapsed": true
      },
      "source": [
        "#PROMPT: Press the play button to set up the technical system (import libraries etc.)\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "print(\"Librosa:\")\n",
        "print(librosa.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2SD2rhaHDF9"
      },
      "source": [
        "# 1. Read audio files\n",
        "\n",
        "Here are a few audio files that come with Librosa. We can use any of them in the subsequent sections. Just remove the hashtag in front of the line to get that audio file.\n",
        "\n",
        "The following code also shows how to select only a _segment_ of an audio file. This is done by keywords `offset` and `duration`. Offset specifies where you want to start the segment (in seconds) and duration (also in seconds) is self-explanatory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trn2ya5RNovU"
      },
      "source": [
        "#filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "#filename = librosa.ex('brahms')    # Hungarian Dance #5\n",
        "filename = librosa.ex('choice')    # A short drum and bass loop\n",
        "#filename = librosa.ex('fishin')    # Karissa Hobbs - Let’s Go Fishin’ A folk/pop song with verse/chorus/verse structure and vocals\n",
        "#filename = librosa.ex('nutcracker')# Tchaikovsky - Dance of the Sugar Plum Fairy\n",
        "#filename = librosa.ex('vibeace')   # 60-second clip\n",
        "x, sr = librosa.load(filename, duration = 20) # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveshow(y=x,sr=sr)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Learning Task 1\n",
        "---\n",
        "Try out some of the audio examples yourself. To select one, remove the hashtag (#) from the beginning of the line. NB. Only one audio example should be run at a time, so make sure that the rest of the 'filename' commands have a # in front of them."
      ],
      "metadata": {
        "id": "G2y2Sd86yV9i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDVjKwaFfM"
      },
      "source": [
        "## 1.1 Load audio files from online sources\n",
        "This is how to load example sound files from Github pages (or anywhere from online, if you know the URL)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgjfvXZaMA3"
      },
      "source": [
        "import soundfile as sf\n",
        "import io\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/clar_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/trumpet_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harp_one_note.wav\"\n",
        "#url = \"https://raw.githubusercontent.com/tuomaseerola/audio/master/harpsichord_one_note.wav\"\n",
        "\n",
        "y, sr = sf.read(io.BytesIO(urlopen(url).read()))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveshow(y = y, sr = sr, color='purple')\n",
        "ipd.display(ipd.Audio(data = y, rate = sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Load audio files from your hardrive / google drive (Optional)\n",
        "It is also possible to load any audio you have on your hard drive to the workspace of the notebook by clicking the folder icon (on the left) and choosing the file, waiting for the file to upload, and then referring to the file. These audio files will **NOT be saved** across the sessions unless you connect your google drive with Colab, which is easy (see [External data: Local Files, Drive, Sheets and Cloud Storage](https://colab.research.google.com/notebooks/io.ipynb)).   "
      ],
      "metadata": {
        "id": "8ChDg4JYzpPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only if you have connected a google drive to the notebook\n",
        "#y, sr = librosa.load('my_own_recording.wav') # if you want to start the segment from a specific point, add offset = [2 for example] in the command\n",
        "#plt.figure(figsize=(12, 4))\n",
        "#ipd.display(ipd.Audio(data = y, rate = sr))"
      ],
      "metadata": {
        "id": "iCk5C6KezLln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwcceopAD2wz"
      },
      "source": [
        "# 2. Explore some acoustic features across time\n",
        "First extract RMS (Root Mean Square) energy and convert this into decibels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoSNdq0BHZP_"
      },
      "source": [
        "filename = librosa.ex('brahms') #PROMPT: Plot the loudness for nutcracker and brahms excerpts. Which one has more dynamic changes?\n",
        "y, sr = librosa.load(filename, duration = 20, offset=2)\n",
        "ipd.display(ipd.Audio(data = y, rate = sr))\n",
        "\n",
        "rms = librosa.feature.rms(y = y)                 # Extra dynamics (RMS)\n",
        "db = librosa.amplitude_to_db(rms, ref = np.max)   # Convert into dB. Note that this is a relative measure (loudest is now 0)\n",
        "times = librosa.times_like(rms)\n",
        "\n",
        "#db = librosa.feature.spectral_centroid(y = y)      # calculate spectral centroid\n",
        "#times = librosa.times_like(rms)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times, db[0],color='black')\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('dB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Learning Task 2\n",
        "---\n",
        "There is another feature called _spectral centroid_ that is often used as a proxy for perceptual brightness that you could visualise across time. Calculation of this is very simple and shown above (commented out). You could explore it or some other features from [spectral features](https://librosa.org/doc/latest/feature.html). Note that the labelling of the Y axis needs to be fixed."
      ],
      "metadata": {
        "id": "lQ06Nkwv272c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql17pRbHbvd"
      },
      "source": [
        "# 3. Analyse pitch\n",
        "This is best done with a monophonic example, although librosa has algorithm to track all pitches as well. Here we extract the fundamental frequency of the trumpet solo using a probabilistic variant of the so-called YIN method. The variant is by Mauch and Dixon (2014) and the original technique was proposed by De Cheveigne and Kawahara (2002)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7ukgJrHd4p"
      },
      "source": [
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording  #PROMPT: Run this code with the trumpet recording\n",
        "y, sr = librosa.load(filename)\n",
        "ipd.display(ipd.Audio(data=x, rate=sr))    # code added to hear audio file\n",
        "\n",
        "# This is where the pyin algorithm is applied. fmin refers to the lower frequency threshold and fmax to higher frequence threshold.\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(y=y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C6'))\n",
        "times = librosa.times_like(f0)\n",
        "\n",
        "# Plot the fundamental frequency\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(times,f0,'ro:', linewidth=2)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Hz')\n",
        "ax=plt.gca()\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())\n",
        "ax.set(yticks=[320,320+80,320+80*2,320+80*3,320+80*4,320+80*5])\n",
        "ax.set(ylim=[300, 800])\n",
        "ax.yaxis.set_major_formatter(librosa.display.LogHzFormatter())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGxCiVtTHfWc"
      },
      "source": [
        "# 4. Analyse Spectrum / Spectrogram\n",
        "\n",
        "Spectrum is a useful representation of audio. Try out plotting the spectrogram with two different audio files from the available ones listed in Section 1.2 (trumpet sound vs example1d).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lazjn7vHjJ9"
      },
      "source": [
        "filename = librosa.ex('trumpet') #PROMPT: Try this out with the trumpet audio example. What is the spectrogram telling you?\n",
        "\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y, sr = librosa.load(filename)  #for the example1d file, put a # in front of this line of code\n",
        "\n",
        "#Nfft = 512\n",
        "stft = np.abs(librosa.stft(y))\n",
        "freqs = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "ipd.display(ipd.Audio(data = y, rate = sr))\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.specshow(librosa.amplitude_to_db(stft, ref=np.max), x_axis='time', y_axis='log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JY4Mgq1B1AT"
      },
      "source": [
        "## PROMPT: Look at the trumpet sound again. Take only the last note. What is the fundamental (F0)?\n",
        "filename = librosa.ex('trumpet')   # Mihai Sorohan - Monophonic trumpet recording\n",
        "y, sr = librosa.load(filename,offset=2.6,duration=0.75) # take the last note\n",
        "stft = np.abs(librosa.stft(y)) # FFT\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(y=y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7')) # Estimate F0\n",
        "f = np.nanmedian(f0)         # Get the Hz of the fundamental frequency for nice labels\n",
        "n = librosa.hz_to_note(f)    # Convert Hz to note name\n",
        "print(f)\n",
        "print(n)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "# collapse across time and plot a spectrum representation (energy across frequencies)\n",
        "Dmean=stft.mean(axis=1)/max(stft.mean(axis=1))\n",
        "plt.plot(freqs,Dmean,color='m')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.xlim([300, 2000])\n",
        "x=np.arange(f,f*10,f) # Create x-ticks based on the F0\n",
        "plt.xticks(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omUNIRYyHhwQ"
      },
      "source": [
        "# 5. Extract onsets\n",
        "This estimates the strength of possible onsets and then detects the onsets that are stronger than a threshold defined in the algorithm. There is also a beat tracking that uses the onsets to estimate beat in the music."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oolz7CufHnOF",
        "collapsed": true
      },
      "source": [
        "## PROMPT: Look at the Brahms example. Take the first 20 seconds (as duration). Afterwards, have a look at the 'brahms' example as well.\n",
        "#filename = librosa.ex('brahms') # vibeace, fishin, or choice\n",
        "filename = librosa.ex('vibeace') # vibeace, fishin, or choice\n",
        "y, sr = librosa.load(filename, duration = 20)\n",
        "\n",
        "# 1. Calculate the strength of the onset candidates ---\n",
        "o_env = librosa.onset.onset_strength(y = y, sr = sr) # calculate onset strength\n",
        "times = librosa.times_like(o_env, sr = sr)\n",
        "\n",
        "# 2. Calculate the onset locations ---\n",
        "onset_frames = librosa.onset.onset_detect(onset_envelope = o_env, sr = sr)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# PLOT\n",
        "plt.plot(times, o_env, label='Onset strength')\n",
        "plt.vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,\n",
        "           linestyle='--', label='Onsets')\n",
        "plt.legend()\n",
        "\n",
        "# 3. Beat tracking ---\n",
        "# note that you can make the beat tracking more loose (0) or tight (100) with tightness or set the starting tempo in bpm\n",
        "tempo, beats = librosa.beat.beat_track(y = y, sr = sr, trim = True, tightness = 90, start_bpm = 120)\n",
        "# Sonify the beats\n",
        "y_beats = librosa.clicks(frames = beats, sr = sr)\n",
        "print(np.round(tempo,1))\n",
        "combined = y[0:len(y)-sr]+y_beats[0:len(y)-sr]\n",
        "\n",
        "librosa.display.waveshow(y=y_beats,sr=sr)\n",
        "ipd.display(ipd.Audio(data=combined, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Learning Task 3\n",
        "---\n",
        "Find **tempo** of the extract in Beats Per Minute (BPM).\n",
        "*Tip. There is a command called* `librosa.feature.tempo` *that can be used to calculate the tempo.*"
      ],
      "metadata": {
        "id": "uP3079Yn5WBU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7zSbYC7NgNS"
      },
      "source": [
        "## PROMPT:Calculate the approximate tempo of the brahms audio example.\n",
        "\n",
        "tempo = librosa.feature.tempo(y = y, sr = sr)\n",
        "print(np.round(tempo,1),'BPM')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show all the candidates from tempo estimation."
      ],
      "metadata": {
        "id": "jThHGzVINkJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute local onset autocorrelation\n",
        "hop_length = 512\n",
        "oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
        "tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,\n",
        "                                      hop_length=hop_length)\n",
        "# Compute global onset autocorrelation\n",
        "ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
        "ac_global = librosa.util.normalize(ac_global)\n",
        "# Estimate the global tempo for display purposes\n",
        "tempo = librosa.feature.tempo(onset_envelope=oenv, sr=sr,\n",
        "                              hop_length=hop_length)[0]\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "times = librosa.times_like(oenv, sr=sr, hop_length=hop_length)\n",
        "freqs = librosa.tempo_frequencies(tempogram.shape[0], hop_length=hop_length, sr=sr)\n",
        "plt.plot(freqs[1:], np.mean(tempogram[1:], axis=1),\n",
        "             label='Mean local autocorrelation')\n",
        "plt.axvline(tempo, color='black', linestyle='--', alpha=.8,\n",
        "            label='Estimated tempo={:g}'.format(tempo))\n",
        "plt.legend(frameon=True)\n",
        "plt.set_xlabel=('BPM')\n",
        "plt.grid(True)\n",
        "ax.set_xlim(50,200)\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "rrIlEJ576pXL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}